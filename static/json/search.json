[
    {
        "uri": "/content/guides/_index",
        "title": "Guides",
        "content": "\nGuides\n\nEach guide in this section documents a set of steps that were taken to solve a problem.\n",
        "tags": []
    },
    {
        "uri": "/content/guides/CICD_Pipelines_TFS_NET_PCF",
        "title": "CI/CD Pipelines for Team Foundation Services and .NET on Pivotal Cloud Foundry Configuration Guide",
        "content": "\nTeam Foundation Server (TFS) is a popular Source Code and Content Management system for companies developing applications with Microsoft technologies. Targeting Pivotal Cloud Foundry (PCF) installations from TFS can be easily accomplished using existing platform capabilities and available marketplace extensions.  This guide is intended for an organization already using TFS and wants to establish a Continuous Integration and Continuous Delivery (CI/CD) pipeline for .NET and .NET core applications deploying into PCF installations.  \n\nSee here for the Google document version.\n\nRequirements\n\nThe following are required to successfully complete the configuration of the CI/CD pipeline in Team Foundation Services instructions found in this document. The steps have been successfully tested in both the public and private installation scenarios.  \n\nTeam Foundation Services 2017 SP1\n\n    Required for building .NET Core applications\n\n    A standalone server installation can be used in conjunction with other source code management systems or older versions of Team Foundation Services\n\nCloud Foundry TFS Extension (available from marketplace)\n\nVisual Studio 2017 for build agents\n\nCloud Foundry CLI for build agents\n\nCloud Foundry Account(s) for deployment\n\n New Windows Build Agents:\n\nCreate build machines and install the build agents following Microsoft’s instructions found here.\n\nInstall the Cloud Foundry CLI for Windows on the build machine found here**.\n\nInstall any additional tools on the build machine that is required to complete code builds.\n\nExisting Windows Build Agents:\n\nInstall the Cloud Foundry CLI on the build machine found here**.\n\n**NOTE: The Cloud Foundry CLI can be pulled down from a repository during the deployment phase as an alternative to installing the CLI on each build machine.  One advantage of pulling the CLI from the repository for each deployment is that it helps ensure the correct version is always used during the deployment process.\n\n Server Endpoints:\n\nTo make PCF API endpoints available to TFS, the endpoints will need to be added to each project.  To add the endpoints, login with using an account with permission to add Services.\n\nTo add a PCF Endpoint:\n\nSelect +New Service Endpoint in the Services page\n\nSelect Generic for the type\n\nIn the dialog box enter the information for the PCF API server endpoint\n\nSelect OK to save the information\n\nServer Extensions:\n\nTo simplify the deployment using the Cloud Foundry CLI, the use of a Microsoft extension available from the marketplace is recommended.  These steps can be accomplished through batch scripts but this approach would require extensive parameterization to support various PCF deployment scenarios.  This Microsoft extension needs to be installed from the TFS Portal and  can be found here.\n\n Building for Cloud Foundry\n\nDuring the build process, changes will need to be made to the build definition. The goal of this step is to publish the application artifacts to a folder rather than the default of creating an IIS installation package.      \n\n.NET 4.52+ Application:\n\nCreate a new build definition and select ASP.NET Template\n\nAttach project to the repository and select \"Create\"\n\nAdd the following arguments to the MSBuild task in the build definition. This will place the publish output into the artifact staging directory.\n\n    **/p:DeployOnBuild=true /p:DeployDefaultTarget=WebPublish\n    /p:WebPublishMethod=FileSystem /p:PackageAsSingleFile=false\n    /p:SkipInvalidConfigurations=true /p:PublishUrl=\"$(build.artifactstagingdirectory)\\\\\"**\n\nAdd a Publish Artifact task if it is not already in the definition\n\nIf not already set, enter $(build.artifactstagingdirectory) in the Path to Publish field\n\nSave the build definition\n\n .NET Core Application:\n\nCreate a new build definition and select ASP.NET Core Template\n\nAttach project to the repository and select \"Create\"\n\nModify the \"Restore\" task to use the .csproj project file instead of the project.json file\n\nModify the \"Build\" task to use the .csproj project file instead of the project.json file\n\nModify the \"Test\" task to use the .csproj project file instead of the “Test” project.json file\n\nModify the \"Publish\" task as follows:\n\n    Uncheck the option for Publish Web Projects\n\n    Uncheck the option for Zip Published Projects\n\nAdd a Publish Artifact task if it is missing from the definition and configure using step 5 from the .NET 4.52+ Application instructions  \n\nSave the build definition\n\nSetting Up Releases\n\nPivotal Cloud Foundry deployments can be accomplished using the Cloud Foundry Extension.  Users have a choice between deploying through a manifest file or through the configuration of the extension.  To use a manifest file that is part of the project, include it with the published output of the project. .NET 4.x applications must run on Windows while .NET Core apps can run on Windows and Linux.\n\nExample project file entry to include a manifest file:\n\nItemGroup\n\tNone Update=\"manifest.yml\"\n\t\tCopyToOutputDirectoryAlways/CopyToOutputDirectory\n\t/None\n/ItemGroup\n\n .NET 4.52+ Application (No Manifest):\n\nCreate a new Release Definition for the project and select Empty template\n\nSelect the project you wish to deploy and hit \"Create\"\n\nRename the default added environment \"Development\" – this environment is setup initially to be triggered whenever a new release is available\n\nSelect the \"Run on Agent\" phase and click “Add Task”\n\nFind the Cloud Foundry push activity under Deploy and click \"Add\"\n\nSelect the endpoint for the deployment\n\nUpdate connection settings to include the organization and space on the PCF installation you wish to deploy\n\nConfigure your deployment options to update the buildpack (e.g. -b hwc_buildpack) and target stack (e.g. -s windows2012R2) information for the application. Note 512Mb for memory limit should only be used as a guide only. Consider starting higher and scaling down to suit application.\n\nSet the working directory for the task to the \"drop\" folder created in the build – Tip: use the helper to locate the correct drop folder from the published artifacts\n\n10. Add additional environments, updating the connection string and deployment options for each environment\n\n11. Save the Release Definition\n\n.NET Core Application (With Manifest):\n\nThe deployment of a .NET Core application is the same as deploying a .NET 4.x application with just one minor modification to the working folder.  Configuring a deployment using a manifest will require adjustments to the deployment options of the extension.\n\nFollow steps 1-7 as above\n\nConfigure your deployment options to specify the manifest file to use for the deployment and other arguments you wish to pass to the CLI – in this case, specifying a different hostname to use and a specific buildpack\n\nUpdate the working directory to point to the directory with the source code published in the .NET Core build definition – Tip: use the helper to locate the correct source folder from the published artifacts\n\nSave the Release Definition\n\n Configuring Blue/Green Deployments\n\nTFS deployments can be created to allow for automated Blue/Green deployments in PCF. The following is a simple example of creating a Blue/Green CI/CD pipeline using TFS and the PCF extension to update routing after successful deployments.  For more information on Blue/Green deployments, see the information located here.  \n\nThe following example is based on two production deployment instances called WingtipToys-green and WingtipToys-blue.  The production hostname is WingtipToys-prod for this example.\n\nGreen Deployment\n\nCreate a new release definition and call it \"Green Deploy\"\n\nComplete the build tasks from the Release section above based on the type of application you are deploying\n\nIn the Deployment Options, set the name and the hostname of the app to include \"-green\" to the end of the name\n\nAdd a CF tasks from the Utility tab to the deployment definition\n\nUpdate the task command to be \"map-route\" and update the arguments to add the production route to the green deployment\n\nThe following can be done as part of the Green Deployment – The second phase in this example was broken out to allow for an approval step\n\nAdd another environment to the Green Deployment and call it \"Post Process\"\n\nSet the connection settings for the org and space in the connection options\n\nAdd a CF tasks from the Utility tab to the \"Post Process\" Run build agent and create a “map-route” task to re-register the blue route so that it can be accessible through a different hostname\n\n10. Add another CF tasks for an \"unmap-route\" task to de-register the blue deployment from the production route\n\n11. Add another CF tasks for an \"unmap-route\" task to de-register the green route\n\n12. Save the deployment definition\n\n Blue Deployment:\n\nClone the \"Green Deploy\" release definition and call it “Blue Deploy”\n\nBased on the steps above, replace the green definitions with the blue definitions and vice-versa in the task definitions\n\nSave the deployment definition\n",
        "tags": [
            "CI/CD",
            "TFS",
            "Configuration Guides"
        ]
    },
    {
        "uri": "/content/main/_index",
        "title": "Basics",
        "content": "\nGetting Started\n\nHere are some useful tips for getting started with .NET on PCF.\n\nDeveloper Getting Started Guide\nTroubleshooting .NET apps on PCF\nWindows Cell Reference Architecture\n\n Configuration Guides\n\nCI/CD Pipelines for TFS and .NET on PCF\n\nRecipes\n\nBrowse some of our application replatforming and modernization recipes for specific technologies, patterns, or application types.\n\nApp Recipes\n",
        "tags": []
    },
    {
        "uri": "/content/main/cell_architecture",
        "title": "Windows Cells",
        "content": "\nWindows Cell Reference Architecture\nWindows stemcells are just like Linux based stemcells with the obvious difference being they run Windows Server instead of Linux. Most of the same terminology, tools, and processes that apply to Linux stemcells also apply to Windows stemcells.\n\nSome terminology you should be familiar with before continuing:\n\nStemcell - a versioned Operating System image wrapped with IaaS specific packaging that serves as a base template for cells.\nCell - an instance of a stemcell or VM running inside Cloud Foundry that runs various container workloads.\n\n What is a Windows Stemcell?\nAccording to bosh.io, a stemcell is simply a versioned Operating System image wrapped with IaaS specific packaging. While that's the most basic definition of a stemcell, there are specific requirements that must be met by a Windows stemcell.\n\nA Windows stemcell contains a bare minimum Windows Server 2012R2 OS with a few Windows features pre-installed, a BOSH agent and some applied policies to securley configure the OS. All Windows stemcells are configured this way regardless of IaaS or who built them. This property of stemcells allows Cloud Foundry operators to easily and reliably deploy to multiple infrastructures without worrying about differences between images.\n\nWindows stemcells do not contain any specific information about any software that will be installed once that stemcell becomes a specialized machine in the cluster. This clear separation between base Operating System and later-installed software is what makes stemcells a powerful concept.\n\nCells as Immutable Infrastructure\nWindows cells are running instances of a specific stemcell version. Think of a stemcell like a static template that all instantiated cells are based off. An important concept that differs from traditional Windows administration is that cells are not long lived \"pets\" that are to be cared for and tended too, rather they are short lived fulfilling a specific purpose more akin to cattle. This means that cell instances come and go as the platform is scaled out or in, updated, or reconfigured.\n\nAnother important distinction between regular Windows Server instances and Windows cells is that Windows cells follow the immutable server pattern. In other words, once a cell instance has been spun up they are not reconfigured, updated, or otherwise changed in anyway. They're not even rebooted! They're essentially read only or immutable for their entire short lived life.\n\n Immutable infrastructure provides stability, efficiency, and fidelity to your applications through automation and the use of successful patterns from programming. No rigorous or standardized definition of immutable infrastructure exists yet, but the basic idea is that you create and operate your infrastructure using the programming concept of immutability: once you instantiate something, you never change it. Instead, you replace it with another instance to make changes or ensure proper behavior.\n\nAn introduction to immutable infrastructure\n\nThe lifetime of a Windows cell varies from cell to cell, but typically this is measured in days instead of months or years. A cells life could be as short as a few minutes to as long as a few months, but typically cells only live for a few weeks before they are replaced because of an updated base stemcell image.\n\n Building a Windows Stemcell\nFor all public clouds, Pivotal provides publicly available Cloud Foundry stemcell images that are already preconfigured using the following best practices:\n\nLatest Windows updates from Microsoft.\nHostable Web Core and supporting features installed.\nLatest .NET 4.x installed.\nLatest BOSH agent.\nRecommended security policies applied.\nDisabling non-essential Windows services and features, including RDP.\nSysprep\n\nFor private infrastructures like vSphere you'll need to build your own stemcell carefully following the same procedures that Pivotal uses internally to build your stemcell. Any deviation from the recommended practices will lead to inconsistencies and ultimately an unsupported configuration that could cause problems for you and your application developers.\n\nThe Windows stemcell builder will work with any supported IaaS, but you should only ever need to run it yourself for private cloud scenarios like vSphere. To get started it's recommended to follow the vSphere manual instructions. These instructions walk you through the process of creating a Windows stemcell including installing and configuring all the prereqs mentioned previously. Once the manual process is understood and proved to be working in your environment, it's recommended to automate the process with a pipeline as you'll want to build stemcells at least monthly to rollout updates.\n\nCorporate \"Golden\" Images\nOften times enterprises have an IT group that is reponsible for producing blessed Windows templates. In some ways what these groups provide is similar to a Cloud Foundy stemcell, however they're usually a bit more heavy weight and aren't necessarily optimized for hosting in the cloud.\n\nWhile you could theoretically use one of these corporate base images as a starting point for your Cloud Foundry stemcells, tread with caution here. These images tend to be bloated and contain many features you shouldn't deploy to Cloud Foundry. Additionally, it's typically better to build stemcells as close as possible to what Pivotal provides for public clouds.\n\n Windows Updates\nEnsuring all your Windows cells have the latest Windows updates is critically important to securing your Cloud Foundry environment. The Windows update process you're probably accustomed to involves WSUS and applying updates to servers during off hours or scheduled maintainence periods to allow for the server to reboot. The process for rolling out Windows updates for Windows cells is very different, and as you'll soon discover much better!\n\nIf you remember from our building a Windows stemcell section, we apply Windows updates when building a stemcell. We don't apply Windows updates to already running cells, instead we replace them with a new base stemcell image that already includes all the patches. Following the immutable infrastructure pattern, each cell instance is then replaced without any downtime and completely managed by Cloud Foundry as it moves workloads from the old running cells to the newly patched cells. This allows patches and all the required reboots to happen once on the base image without needing to do it across hundreds or even thousands of servers.\n\nSince Microsoft typically releases new Windows updates monthly on patch tuesday it's recommended to follow that schedule to build a new stemcell and replace all of your Windows cells. While you may have the ability to replace all your production cells on patch Tuesday, it's recommened that you first deploy newly built stemcells to your pre-production Cloud Foundry environments and let those bake for a few days before promoting them to your production Cloud Foundry foundations. This will give the apps running on those foundations a chance to validate the patches haven't broken anything in any of the applications which are running within the containers on it.\n\nCell Customization with BOSH Addons\nThe best practice for installing additional software or applying custom configuration is to create a BOSH addon. This includes additional Windows features, policies, and 3rd party software often baked into corporate golden images.\n\nIf you're not familiar with BOSH, it is the underlying technology that deploys Cloud Foundry and all of it's components - including Windows cells. A BOSH addon is an extensibility point to a stemcell that allows the Cloud Foundry operator to inject custom software or configuration in a consistent and repeatable manner across all cells and IaaSs. Using the BOSH runtime config a addon can even be targeted to a specific OS, like Windows, so an addon only runs on a compatible OS.\n\nWhen installing additional Windows software with a BOSH addon it is extremely important that the installation can run unattended or in silent mode without user intervention. An addon runs everytime a new cell is spun up from a stemcell, so it's important to follow these addon guidelines:\n\nSoftware installation should not require large binaries.\nSoftware installation should be fast.\nSoftware installation must not cause a reboot.\nInstallations must run headless without user intervention.\nInstallers must run locally as they run under the BOSH local SYSTEM account.\n\nAdditional Windows features, configuration, security, or local group policy should be applied using a BOSH addon for the same reasons as software installation, to decouple cell specialization from stemcell building and infrastructure.\n\n Windows Cell Anti-Virus\nWhile AV software isn't supported out of the box with Cloud Foundry Windows cells, it is possible to install and configure AV software if it's required by your corporate security standards.\n\nIt's important when creating a new BOSH addon to install your AV software that the installation can be performed silently without user intervention. It's also important to configure the AV agent so that any on-access scanners exclude some of the underlying Cloud Foundry runtime directories, otherwise intermittment deployment failures may occur. The following directories should be excluded from on-demand scanning:\n\nC:\\bosh\nC:\\var\\vcap\nC:\\containerizer\n\nFailure to exclude these directories may lead to ephemeral permission issues with files in these folders and cause cell and app deployments to fail.\n\nActive Directory\nActive Directory is a powerful tool that many enterprises leverage in order to more easily manage their corporate servers and workstations. Additionally AD provides a centralized way to manage and authenticate users within an intranet. While AD is very good at these tasks, it wasn't designed during a time when cloud computing existed. This can be seen in many of the design and architecture choices baked into AD and how those designs struggle at times to support ephemeral immutable infrastructure like Cloud Foundry.\n\nWhile it's technically possible to join a Windows cell to an AD domain, it is absolutely not supported by Cloud Foundry. In addition to not being supported it is also highly discouraged for multiple reasons. Aside from that, it's better to take a step back and ask, \"what problems are you trying to solve with Active Directory?\" Generally speaking most issues that Windows admins are trying to solve with AD either don't exist in Cloud Foundry or there are other ways to solve using BOSH specific cloud friendly paradigms.\n\nActive Directory is typically meant for long lived servers that are treated well and maintained as pets. Often times administrators may need to RDP into these servers in order to perform maintenance. In Cloud Foundry this couldn't be further from the truth. Administrators cannot login to cells to perform maintenance, in fact WinRM and RDP are both disabled by the stemcell builder. Instead of \"maintenance\" at the cell level you would put the cell in question down, e.g. destroy it and spin up a brand new healthy cell from a new stemcell. If there's no need to login to a cell then there's no need for a domain admin group to be added to the local administrators group on the cell.\n\nYou might think that you may need access to a stemcell for troubleshooting, but in general there are other ways to troubleshoot cells without logging in. Applications which run on Cloud Foundry have their logs collected in the centralized Cloud Foundry loggregator logging system. Even Windows event logs are collected in the loggregator should you need to troubleshoot a systemic cell configuration issue.\n\nNOTE - .NET applications cannot use integrated authentication even if you were to domain join the cell. The hostable web core provided by Cloud Foundry explicitly disallows Windows auth even if it's been \"enabled\" in the application's web.config.\n\nAnother difference between the way BOSH managed cells and AD managed servers is that it's considered best practice to automate, version control and test all changes in non-production environments before rolling them out to production environments. Typically Active Directory configuration is applied at the domain or OU level manually thus potentially leading to inconsistent configuration between sites. Additionally this type of configuration is difficult to version control and validate when compared to a version controlled BOSH addon. While you could do the same with PowerShell and some discipline, following the BOSH way forces you to use best practices.\n\nLet's say you're not convinced, and you still want to domain join your cells. The first issue you'll run into is with BOSH not tolerating reboots during deployment. This means you'll need to domain join the cells outside BOSH or some other fragile technique. Assuming you solve that there are additional issues you'll likely encounter:\n\nBOSH doesn't tolerate reboots and will fail cell deployment.\nAD group policy could cause the Cloud Foundry services on the cell to fail in unexpected ways.\nGarbage collecting old computer accounts from AD as cells are dynamically spun up and down.\n\nWhile domain joining a cell may seem like a shortcut to applying your existing security policies and configuration, it's a shortcut fallacy that will cost you more time in troubleshooting and pain then it's worth. It's far better to bite the bullet and do things the BOSH way.\n\n FAQs\n\nCell Sizing\nThe recommended cell size depends on the underlying infrastructure and other factors, however a good starting point is:\n\n4 CPUs\n16GB RAM\n128GB disk\n\n Pagefile\nThe stemcell builder automation doesn't currently configure the Windows pagefile, so each of yours cells will use the Windows default settings which is \"Windows Managed\". This setting usually works well for Windows cells, however if you plan on changing this it's probably best to do this using Microsoft's guidance to support your cell size.\n\nD: drive?\nWindows cells do not have multiple paritions or drives. Traditionally with bare metal Windows Servers you would reserve the C: drive for the OS and D: drive for programs etc. With completely virtualized environments along with the short lived nature of Windows cells this advice no longer provides any benefits. While you could add another drive to your stemcell buildout process, it's not recommended or supported.\n\n .NET Versions\nIt's recommended that the latest version of .NET be installed on your stemcell. Unfortunately the way the .NET 4.x framework and CLR are installed is globall at the cell level. While the mutlitude of .NET versions are generally backwards compatible, applications pushed to Cloud Foundry can only target the latest version already deployed on the cell. This is different for .NET Core where the CLR version is independently deployed along with the application and doesn't require that it be preinstalled by a Cloud Foundry operator.\n\nRemote Cell Access\nRemote cell access via RDP or WinRM is disabled by default by the stemcell builder. This is generally the best option from a security and management standpoint. Generally all troubleshooting should be done during the stemcell buildout process or via the Cloud Foundry logs. At some future incarnation Windows cells may support operator access via bosh ssh. If you really need console access to a cell you may look at using the vSphere console to login or create a bosh addon to enable RDP or WinRM. If you do enable this type of access it's recommended you only do so temporarily and/or in non-production environments.\n",
        "tags": [
            "architecture",
            "stemcells",
            "bosh"
        ]
    },
    {
        "uri": "/content/main/getting_started",
        "title": "Getting Started",
        "content": "\nGetting started pushing ASP.NET 4.x apps to PCF\n\nBefore pushing your first ASP.NET application to Pivotal Cloud Foundry you should follow the below steps in order to ensure you have a positive experience running your applications in PCF.\n\n Application Manifest\n\nAn application manifest is a YAML file which contains information about how to host your application on PCF. A good starting manifest for the majority of ASP.NET apps is below, copy and paste this into the same directory as your Web.Config and name it manifest.yml. Replace the REPLACE_ME text in the manifest with the name of your app.\n\n---\napplications:\nname: REPLACE_ME\n  memory: 4G\n  stack: windows2012R2\n  buildpack: hwc_buildpack\n  health-check-type: port\n\nThis application manifest is minimal, but specifes a few important details for hosting an ASP.NET app in PCF. It specifies that the container RAM should be 4GB, the app should run on a Windows 2012 R2 cell, and use the Hostable Web Core to start the application (HWC is what IIS uses to run apps under the covers). There are many more tunables available in the application manifest you may want to tweak later on.\n\nNOTE - Don't set the application memory below 4G when first pushing the app to PCF. Once you have the app successfully running it's recommended to then go back and tune the memory down.\n\nApplication Error Handling\n\nTo take advantage of the builtin PCF logging your application needs to 'opt-in' to ensure you can properly view any unhandled exceptions that may occur in your app. This is a very important step and should be done before pushing the application for the first time. Without a global error handler in place you won't get any log information back from your application if it crashes on startup.\n\nThe easiest way to log any unhandled exceptions is to modify your application's Global.asax.cs and add a global error handler method that logs to stdout.\n\nvoid Application_Error(object sender, EventArgs e)\n{\n    Exception lastError = Server.GetLastError();\n    Console.WriteLine(\"Unhandled exception: \" + lastError.Message + lastError.StackTrace);\n}\nIf you already have a global exception handler you'll need to ensure it's logging to stdout. For example if using a logging framework like log4net you'll need to configure a console appender so log statements write to stdout.\n\nNOTE - For production applications it's highly recommended to use a configurable logging framework like log4net or NLog instead of writing directly to the Console.\n\nOnce you have added a global error handler you'll see any unhandled execptions along with their stack trace logged to PCF. You can view the logs in the PCF Apps Manager or from the command line: cf logs myappname --recent\n\nNOTE - Do not leave custom errors mode set to off outside development environments as this may leak sensitive information to an attacker.\n\n Web.Config\n\nMost application Web.configs work out of the box with PCF, however here are a couple of things to watch out for.\n\nDon't use Windows integrated auth, it's been disabled in PCF.\nDon't use HTTP modules that don't ship with .NET or can't be deployed in your app's bin directory, for example the Micorsoft URL Rewrite module.\nSQL Server connection strings must use fully qualified domain names.\n",
        "tags": [
            "replatforming",
            "asp.net"
        ]
    },
    {
        "uri": "/content/main/troubleshooting",
        "title": "ASP.NET Troubleshooting",
        "content": "\nThis covers some basic troubleshooting techniques to use for any full .NET application deployed to a Windows stack.\n\nDeveloper Troubleshooting\nAssuming you've followed the recommedations for logging and error handling in the getting started guide you shouldn't have a hard time diagnosing issues with your deployed .NET app. Below are some common failures and possible corrective actions that you as a devloper may take to fix the issue.\n\n StackOverflowException, AccessViolationException\nIf you see one of these exceptions in the PCF logs, but not locally, it usually means your application is getting killed by the PCF runtime because it's attempting to use more RAM than has been allocated to the container. Check your manifest.yml and bump up the memory setting, 4G is a good starting point.\n\n502 Gateway Error\nIf your application is reporting back a 502 Gateway Error, this will happen if the HWC process has crashed for some reason but the container healthcheck hasn't detected an unresponsive application. Fix the root cause, usually an unhandled exception in your application which should show up in the PCF logs. You should also change your health-check-type to http so that the PCF runtime can detect your app has crashed and automatically restart it.\n\n ASP.NET Yellow Screen of Death\nWhen you see an ASP.NET Yellow Screen of Death (YSoD) this means the application cannot start because of a misconfiguration or has crashed. Typically you'll want to check your PCF application logs when you see a YSoD, assuming you've put a global error handler in place.\n\nThere are times when it's necessary to temporarily configure the ASP.NET yellow screen of death to output errors directly to the browser. These scenarios include:\n\nThe application contains a configuration error, typically in the web.config.\nYou lack the ability to modify the application and add a global error handler method.\n.NET cannot start the app domain.\n\n Outputting detailed error messages to the browser can be enabled by modifying the Web.config and setting the customErrors mode attribute to Off, like so:\n\nsystem.web\n  customErrors mode=\"Off\"/\n/system.web\n\nThis will allow you to see the underlying error message and stack trace from your browser and is typically the first debugging action you'll take after checking the PCF logs. If you added a global error handler you will also see the underlying exception in your PCF logs.\n\nNOTE - Do not leave custom errors mode set to off outside development environments as this may leak sensitive information to an attacker.\n\nApp won't start, reports crashed\nThis will typically only happen if you have set a health-check-type of http and your application has a problem. You should be able to find an error in your PCF logs or on the YSoD page. Another possibility is if your application takes a long time to start before it starts returning responses, look into improving the app startup time.\n\n Logs don't show up in PCF\nThe default in ASP.NET is to not log anything to stdout, and thus PCF. If it does \"log\" anything it'll log fatal errors to the Windows event log or show the yellow screen of death page. You'll need to instrument your code with Console.WriteLine calls or better yet use a logging framework like log4net and configure it to log to stdout.\n\nThe page cannot be displayed because an internal server error has occurred.\nThis usually means the app crashed before it even attempted to execute your code while parsing the web.config.\n\nFirst make sure you are pushing a .NET 4.x app and not a .NET 2, 3, or 3.5 app. If you are targeting an older framework version then upgrade the application to work with .NET 4.x.\n\nIf you're ASP.NET 4.x won't start, try pushing your application with a clean or empty Web.config and then add in bits and pieces of configuration one block at a time until you can narrow down what's not working. Typically it's because of a http module or handler that is not available on the Windows cell, like the Microsoft URL Rewrite module.\n\n .NET Tracing\nASP.NET tracing enables you to view diagnostic information about requests for an ASP.NET page. ASP.NET tracing enables you to follow a page's execution path, display diagnostic information at run time, and debug your application. ASP.NET tracing can be integrated with system-level tracing to provide multiple levels of tracing output in distributed and multi-tier applications. You can also configure ASP.NET tracing to write to the PCF application log.\n\nAll the same .NET tracing rules and configurations that you are familiar with apply to ASP.NET applications hosted in PCF. To get all diagnostic traces from the .NET framework, libraries, and ASPX pages/views to show up in PCF there are a few steps you need to take.\n\nTo get ASPX page level trace statements to show up along with the other trace events you need to add the following to your web.config:\nsystem.web\n  trace enabled=\"true\" writeToDiagnosticsTrace=\"true\" mostRecent=\"true\" pageOutput=\"false\" /\n/system.web\n\nThe most important step is to add a console appender so the PCF logging system can pickup all the trace statements. Additionally you can configure .NET diagnostic information to output the PCF log by enabling their source and log level. The following example will configure all ASPX trace statements and the .NET framework System.Net namespace to output to the PCF log:\nsystem.diagnostics\n  sharedListeners\n    add name=\"PcfLogListener\" type=\"System.Diagnostics.ConsoleTraceListener\" /\n  /sharedListeners\n  sources\n    source name=\"System.Net\"\n      listeners\n        add name=\"PcfLogListener\"/\n      /listeners\n    /source\n  /sources\n  trace autoflush=\"false\" indentsize=\"2\"\n    listeners\n      add name=\"PcfLogListener\"/\n    /listeners\n  /trace\n  switches\n    add name=\"System.Net\" value=\"Information\" /\n  /switches\n/system.diagnostics\n\nThe System.Net trace statements are in particular valuable if you're trying to debug WebRequest failures that your application may be making to other services. Together these tracing techniques can be invaluable in diagnosing issues that may only happen in a limited access environment.\n\nCloud Foundry Operator Troubleshooting\nThis section covers advanced techniques that may come in handy to a platform operator that has administrative access to the Cloud Foundry platform and cells, but potentially limited access to the application code.\n\n Find the cell(s) an application is running on\nFirst get the GUID of the app:\n\n$ cf app APPNAME --guid\nc6d1259c-8057-489e-9ac2-beaa896c2bf3\n\nThen use cf curl and jq to extract the information from the stats endpoint:\n\n$ cf curl /v2/apps/c6d1259c-8057-489e-9ac2-beaa896c2bf3/stats | jq 'with_entries(.value = .value.stats.host)'\n{\n  \"0\": \"10.0.33.4\",\n  \"1\": \"10.0.34.4\"\n}\n\nUse the bosh vms command to correlate the IPs to BOSH job indexes or VM UUIDs.\n\nDownload application droplet\nIf you need to look at what an app has deployed to PCF, you can download the droplet image from PCF. This allows you to take a look at the Web.config or other compiled bits.\n\n$ cf app myappname --guid\n9caddd73-706c-4f82-bb63-b1435bd6240d\n\nCreate a tmp dir, download the container to the tmp dir, and extract the downloaded tar file.\n\n$ mkdir /tmp/droplet && cd /tmp/droplet\n$ cf curl /v2/apps/9caddd73-706c-4f82-bb63-b1435bd6240d/droplet/download  droplet.tar.gz\n$ tar -zxf droplet.tar.gz\n\nIf you list the files in the application you'll see the image looks very much like a deployed application would in IIS. Now you can verify the image layout and the contents of the Web.config without needing to login to the cell or gain access to the source code.\n\n$ ls -lah app\ntotal 128\ndrwxr-xr-x  16 sneal  wheel   544B Apr 20 08:20 .\ndrwxr-xr-x   4 sneal  wheel   136B Apr 21 10:20 ..\ndrwxr-xr-x   4 sneal  wheel   136B Apr 20 08:20 .cloudfoundry\ndrwxr-xr-x   7 sneal  wheel   238B Apr 20 08:20 Bin\n-rw-r--r--   1 sneal  wheel   5.5K Apr 20 08:20 Default.aspx\n-rw-r--r--   1 sneal  wheel   1.2K Apr 20 08:20 Global.asax\n-rw-r--r--   1 sneal  wheel   1.2K Apr 20 08:20 Web.config\ndrwxr-xr-x   4 sneal  wheel   136B Apr 20 08:20 css\n-rw-r--r--   1 sneal  wheel    17K Apr 20 08:20 favicon.ico\ndrwxr-xr-x   4 sneal  wheel   136B Apr 20 08:20 img\ndrwxr-xr-x   7 sneal  wheel   238B Apr 20 08:20 js\n",
        "tags": [
            "troubleshooting",
            "asp.net"
        ]
    },
    {
        "uri": "/content/recipes/_index",
        "title": "Recipes",
        "content": "\nRecipes\n\nEach recipe in this section documents a set of steps that were taken to solve a problem.\nRecipes are intended for modifications that are repeatable and likely to be encountered across applications.\n",
        "tags": []
    },
    {
        "uri": "/content/recipes/azure_db_migrate",
        "title": "SQL Server DB Migration to Azure",
        "content": "\n \n\n \nProcess Description\n \n Create Azure PCF Service\nFirstly, a target SQL Server database in azure must exist that is provisioned by the Azure SQL Server service broker.   \n \nBelow is a Service Creation Pipeline Bash Script Template which demonstrates how to automate the CF CLI to create the SQL Service via the broker,  a JSON Service Configuration File Template which provides the specifics of the database to the CLI, and a Service Creation Concourse Task Template which triggers the script as part of the deployment process. Note that arguments likely to be replaced for your specific case are surrounded by less-than and greater-than signs i.e. SAMPLE ARGUMENT.\n \nNote that this process is engineered to NOT overwrite or destroy the underlying database/service if it already exists.\nSQL server Service broker will automatically create username and password for application when it binds to the service.\n \nJSON Service Configuration File Template For Azure SQL DB\n\n{\n  \"resourceGroup\": \"AZURE RESOURCE GROUP\",\n  \"location\": \"eastus\",\n  \"sqlServerName\": \"SERVER NAME\",\n  \"sqldbName\": \"DATABASE NAME\",\n  \"sqldbParameters\": {\n      \"properties\": {\n          \"collation\": \"SQLLatin1GeneralCP1CI_AS\"\n      }\n  }\n}\n \nService Creation Pipeline Bash Script Template\n\n!/bin/bash\nset -o errexit\nset -o xtrace\n \ncf login -a API URL -o ORG -s SPACE -u $user -p $password\ncf create-service azure-sqldb PLAN SERVICE NAME -c JSON SERVICE CONFIG FILE PATH|| echo \"Already Exists\"\n\nService Creation Concourse Task Template\n\nplatform: linux\n \nimage_resource:\n  type: docker-image\n  source: {repository: czero/cflinuxfs2}\n \ninputs:\n  name: RESOURCE resource containing the JSON Service Configuration File\n\nrun:\n  path: sh\n  args:\n  -exc\n  |\n    ls -lR  \n    sh SERVICE CREATION PIPELINE BASH SCRIPT\n  \nBackup DB\nAsk DBAs for a backup of the on-prem database to be ran and placed on the jumpbox\n\n Restore DB\nFollow Microsoft instructions to [restore DB to Staging Server] (https://msdn.microsoft.com/en-us/library/ms177429.aspx)\n \nSanitize DB\nEach table has to have clustered indexes or pk\nDelete Windows users\n\n Generate BACPAC\nGenerate  BACPAC DB from clean DB in Staging Server (must have admin user credentials)\n\nCommand\nsqlpackage.exe /Action:Export /ssn::DATABASE HOSTNAME /sdn:DATABASE NAME /su:ADMIN USER ID /sp:ADMIN PASSWORD /tf:PATH TO BACPAC FILE\n\nRestore BACKPAC\nRestore BACPAC from file into Azure DB. It Requires server admin role\nCommand\nsqlpackage.exe /Action:Import /tsn:DATABASE HOSTNAME /tdn:DATABASE NAME /tu:ADMIN USER ID /tp:ADMIN PASSWORD /sf:PATH TO BACPAC FILE\n\nFor more information \nAzure IMPORT BACPAC\n",
        "tags": [
            "replatforming",
            "mvc",
            "asp.net",
            "Concourse",
            "msbuild"
        ]
    },
    {
        "uri": "/content/recipes/biztalk",
        "title": "Biztalk",
        "content": "\nBiztalk\n",
        "tags": [
            "modernization"
        ]
    },
    {
        "uri": "/content/recipes/classic_asp",
        "title": "Classic ASP",
        "content": "\n\nActive Server Pages (also known as ASP or classic ASP) is Microsoft's first server-side script engine that enabled dynamically-generated web pages. ASP is Microsoft's alternative to CGI scripts and Java Server Pages typically written in VBScript.\n\nBackground\n\nSince ASP apps are written in VBScript they are slower to execute and respond than their modern compiled counterparts like ASP.NET. For performance critical areas ASP apps typically consume COM components written in a compiled language like C++ or VB6, both of which have their own mantainability issues. Worse yet, COM components lack a cloud friendly deployment model (although this may improve with Windows 2016 containers).\n\nIt's not uncommon for these types of applications to contain a tangle of HTML, CSS, JavaScript, and server side VBScript. ASP applications typically lack composability, structure, and automated tests which make them highly brittle and difficult to replatform. They also predate cloud computing and common security practices and because of this are typically insecure compared to modern web applications.\n\n Labs\n\nWhile it is theoretically possible to run simple ASP applications on PCF, it's actively discouraged. We are Pivotal, therefore we should be helping customers transform how they build software. The recommendation for customers with ASP applications is a Pivotal Labs engagement to replace their existing application with a modern up to date technology, architecture, and design.\n\nThis provides the best chance for success with customers not only from a technology standpoint, but from a transformation standpoint. Without this level of change, oranizations will not be able to leverage their investment in PCF.\n\nModernization\n\nIf the customer cannot commit to rebuilding an application but they really need it to run alongside their other apps in PCF,it makes sense to bring in a consulting firm that specializes in those types of ASP to ASP.NET migrations. There are paid tools like gmStudio which help assist in the migration, but even with good tools there's a lot of work involved and no automation is going to completely work without manual intervention.\n\nThese type of engagements tend to be protracted, add little value to an organization in the context of learning opportunities and represent a large amount of grunt work which can be fulfilled less expensively outside Pivotal.\n",
        "tags": [
            "modernization",
            "asp",
            "asp.net"
        ]
    },
    {
        "uri": "/content/recipes/concourse_build",
        "title": "Concourse Pipeline for .NET Applications",
        "content": "\n\nTo build .NET Applications on Concourse Windows worker with .Net framework and build tools should be installed and configured. Refer to Chris Umbel Blog for details on setting up Bosh managed Concourse workers and sample pipeline.\n\nPipeline and generic build powershell file below could be reused to build and publish .NET applications to PCF.\n\n  \n\nThe sequence of events is:\nGet, compile and unit test (xunit) the solution\nDeploy to PCF\nRun integration (smoke) tests using postman\n\n.NET Project setup\nCI headless builds for NET applications are based on command line interface for Microsoft build tools - msbuild.exe\n\nWe are providing reusable powershell build script that\ninstalls Nuget to retrive nuget packages\ninvokes msbuild to compile the solution\ninvokes xunit runner to test the applications\ninstall web.target and uses Web Publishing profile to publish (assemble) all required assets (dlls, svc, pages etc) to some file directory\nPush all prepared (published) assets to PCF\n\nTo reuse this powerful script in consistent manner Solution must adhere to the following requirements  \n\nHave \"Test\" configuration - builds projects required for unit testing\nHave \"Release\" configuration - build projects that will be deployed to PCF\nWeb based projects have Publishing profile - \"CF\" - which copies all assets to file directory\nProject has .nuget directory with nuget.exe and nuget.config pointing to repository for package installs\n\n Concourse tasks\nExample of full pipeline could be found at NET Pipeline\nMain tasks to build and unit test the solution utilize the build powershell\n\nUnit test\nUnit testing tasks running on windows invokes build.ps1 in test mode\n\njobs:\nname: unit-test\n  public: true\n\n  plan:\n  get: code\n    trigger: true\n\n  task: build-and-test\n    config:\n      platform: windows\n      run:\n        path: powershell\n        args:\n        ./build.ps1\n        -mode test -targetFramework 4.6.1\n        dir: code\n      inputs:\n      name: code\n \n In test mode build script will\n\n Build solution \"Test\" Configuration\n\n   Build the solution, return\nfunction Build-Solution($configuration) {\n\n    $code = -1\n    . {\n        # install web.targets\n        Configure-Publish\n\n        $frameworkVer, $frameworkPath = Get-TargetFramework\n\t\t    $frameworkParam = \"\"\n        if($frameworkVer -ne $DefaultFramework) {\n            $frameworkParam = \"/p:FrameworkPathOverride=\"$frameworkPath`\"\"\n        }\n\n        $app = \"$MsBuildApp /m /v:normal /p:Platform=$Architecture /p:Configuration=$configuration /nr:false $publish $tools $frameworkParam\"\n\n        Write-Host \"Running the build script: $app\" -ForegroundColor Green\n        Invoke-Expression \"$app\" | Write-Host\n        $code = $LastExitCode\n    } | Out-Null\n\n    if($code -ne 0) {\n        Write-Host \"Build FAILED.\" -ForegroundColor Red\n    }\n    else{\n        Write-Host \"Build SUCCESS.\" -ForegroundColor Green\n    }\n\n    $code\n}\n\nRun Unit tests - install xunit runner package, and run all unit test projects ending with *Tests\n\nfunction main {\n    Write-Banner\n\n    $buildConfig = if ($(Get-Mode) -eq 'test') {'Test'} Else {'Release'}\n    $buildResult = Build-Solution $buildConfig\n\n    if($buildResult -ne 0) {\n        Write-Host \"Build failed, aborting...\" -ForegroundColor Red\n        Exit $buildResult\n    }\n\n    if($(Get-Mode) -eq 'test') {\n        Write-Host \"Starting unit test execution\" -ForegroundColor Green\n        NuGet-Install 'xunit.runner.console' $XUnitVersion\n\n        $failedUnitTests = 0\n\n        #Get the matching test assemblies, ensure only bin and the target architecture are selected\n        $testfiles = Get-ChildItem . -recurse  | where {$.BaseName.EndsWith(\"Tests\") -and $.Extension -eq \".dll\" `\n            -and $.FullName -match \"\\\\bin\\\\\" -and $.FullName -match \"$Architecture\"  }\n\n        #Execute unit tests in all assemblies, continue even in case of error, as it provides more context\n        foreach($UnitTestDll in $testfiles) {\n            Write-Host \"Found Test: $($UnitTestDll.FullName)\" -ForegroundColor Yellow\n            Invoke-Expression \"$XUnitApp $($UnitTestDll.FullName)\"\n\n            if( $LastExitCode -ne 0){\n                $failedUnitTests++\n                Write-Host \"One or more tests in assembly FAILED\" -ForegroundColor Red\n            }\n            else\n            {\n                Write-Host \"All tests in assembly passed\" -ForegroundColor Green\n            }\n        }\n\n        if($failedUnitTests -ne 0) {\n            Write-Host \"Unit testing for $(Get-Mode) configuration FAILED.\" -ForegroundColor Red\n\n        } else {\n            Write-Host \"Unit testing for $(Get-Mode) configuration completed successfully.\" -ForegroundColor Green\n        }\n        Exit $failedUnitTests\n    }\n\n}\n\nPublish .NET Project\nPipeline tasks to run Publishing Profile for .NET project and deploy to PCF\nname: publish\n  public: true\n  plan:\n  get: pipeline\n    trigger: true\n  get: code\n    passed:\n    unit-test\n    trigger: true\n\n  task: build-and-publish\n    config:\n      platform: windows\n      run:\n        path: powershell\n        args:\n        ./build.ps1\n        -mode publish\n        dir: code\n      inputs:\n      name: code\n      outputs:\n      name: publish\n\n  put: pcf\n    params:\n      manifest: pipeline/dev/manifest.yml\n      path: publish/{{publishprofiledirectory}}\n\nPowershell script to set publish profile which is invoked by msbuild.exe\nfunction Configure-Publish {\n    . {\n\n        <\n            Always download this package and set up the override for the\n            VSToolsPath as there may be dependencies in the MSBuild file on a\n            specific version on VisualStudio\n        #\n        Write-Host \"Configuring publish support.\" -ForegroundColor Green\n        NuGet-Install 'MSBuild.Microsoft.VisualStudio.Web.targets' $PublisherVersion\n        $script:tools = \"/p`:VSToolsPath=$PublisherPath\"\n\n        if($(Get-Mode) -eq 'publish') {\n            $pre = '/p:DeployOnBuild=true`;PublishProfile='\n            $script:publish = \"$pre$PublishProfile\"\n            Write-Host $publish -ForegroundColor Yellow\n        }\n        Write-Host $tools -ForegroundColor Yellow\n        Write-Host \"Publish support configured.\" -ForegroundColor Green\n    } | Out-Null\n}\n\n[winworkers]: http://www.chrisumbel.com/article/windowsworkertoboshdeployed_concourse \"Concourse Windows Workers\"\n[fullpipe]: https://github.com/pivotalservices/Manulife-App-Replatforming/blob/master/net-libraries/CloudFoundry-Security/ci/pipeline.yml \"Full .NET pipeline\"\n[buildps]: https://github.com/pivotalservices/Manulife-App-Replatforming/blob/master/net-libraries/CloudFoundry-Security/build.ps1 \"Build script\"\n[msbuid]: https://msdn.microsoft.com/en-us/library/ms171452(v=vs.90).aspx\n",
        "tags": [
            "replatforming",
            "mvc",
            "powershell",
            "Concourse",
            "msbuild"
        ]
    },
    {
        "uri": "/content/recipes/config_vcap",
        "title": "PCF Services Configuration",
        "content": "\n.NET Applications and VCAPS_SERVICES\n\nApplications running at PCF leverage Services for environment specific settings.\nAll web.config settings that are specific to environment are moved into CF services that are bound to the application. There services provided by PCF Service Brokers or User Provided Services.\n \nTo enable .NET applications to read  and use CF services information Configuration library is provided. Library will use web.config settings if it detects that it runs in NON-Cloud and CF services informatcion if run on PCF. It will essentialy parse VCAPS_SERVICES json information and provide it as set of classes for the application to use.\n\nThe concept and structure is borrowed from Steeltoe and Spring CloudConnectors.\n \n\n Configuration\n \nInstall  Configuration Library\n\nInstall-Package Pivotal.Configuration.CloudFoundry\n\nThis library reads configuration and provides it as set of classes for the application. It runs seamlessly locally by reading web.config sections and on PCF reading VCAP_SERVICES.\n \nUsage:\n\nConfigurationBuilder config = ConfigurationBuilder.Instance();\n// Get specific service by name and type\nSqlServerService svc = config.GetServiceSqlServerService(\"name-db\");\nSSOService sso= config.GetServiceSSOService(\"Internal-SSO\");\n \n// Get all services of specific type\nListSqlServerService svcs = config.GetServicesSqlServerService();\n \n \n// For generic user provided services\nService svc = config.GetServiceByName(\"servicename\");\n// get config values\nsvc.Credentials[\"value_key\"];\n\nFollowing are Classes Hierarchy available for usage:\n\n \n[conflib]: https://github.com/pivotalservices/Manulife-App-Replatforming/tree/master/net-libraries/CloudFoundry-Config \"Config Library\"\n\n \n\n \n \n \n \n \n",
        "tags": [
            "replatforming",
            "mvc",
            "asp.net"
        ]
    },
    {
        "uri": "/content/recipes/console_app",
        "title": "Console Applications",
        "content": "\nConsole Applications\n",
        "tags": [
            "modernization"
        ]
    },
    {
        "uri": "/content/recipes/ibmmq",
        "title": "IBM MQ",
        "content": "\nIBM MQ recipes for PCF\n\nIBM MQ requires a client library to be installed on the remote server which wouldn't be a recommended approach on PCF.\n\nThere are two alternatives:\n\nMQTT (MQ Telemetry Transport)\n\n    MQTT was developed by IBM as a lightweight network protocol for pub/sub messaging between devices and IBM MQ. Originally intended for lightweight clients like sensors. MQTT is available as an additional service on MQ since version 7. Once the service is running MQTT clients connect and register a clientid and can either publish messages and/or subscribe to a TOPIC. Messages can be sent to the MQTT client via the normal (non MQTT) TOPIC publish channels using the TOPIC string registered by the MQTT client or the clientid.\n\n    A REMOTE QUEUE can also be defined within MQ that forwards messages to the MQTT client. This should mean most existing MQ message flows can be modifyed to support an MQTT client.\n\n    MQTT supports durable messaging along with three QoS such as deliver at least once, deliver only once, deliver exactly once.\n    Higher levels of QoS are more reliable, but involve higher latency and have higher bandwidth requirements.\n\n    *0*: The broker/client will deliver the message once, with no confirmation.\n\n    *1*: The broker/client will deliver the message at least once, with confirmation required.\n\n    *2*: The broker/client will deliver the message exactly once by using a four step handshake.\n\n    For .NET there are several packages available with M2MQTT seemingly being the best supported and maintained although it has a few querks. M2MQTT is a nuget package and requires no client to be installed. There is a dotnet core version but I have not tested it.\n\n    Topic string length is limited to 65536 bytes (utf-8) and message payload is limited to 268,435,456 bytes as per MQTT spec.\n\nAMQP 1.0\n\n    IBM MQ 8 introduced support for AMQP 1.0 with an additional service pack and AMQP is standard on v9 and above.\n\nThere are many AMQP 1.0 libraries available to .NET and dotnet core. These libraries are more recent and better maintained than any of the MQTT libraires. If the customer is running IBM MQ 8 or above AMQP might be the better solution.\n\n Resources:\n\nGood video on setting up a MQ lab on Red Hat Enterprise using IBM MQ for Developers (all free).\nWhat is MQTT and how does it work with Websphere MQ\nIBM MQ Developer Downloads\nSending a test message to a MQTT Client via WebSphere Explorer\n",
        "tags": [
            "MQ",
            "replatforming"
        ]
    },
    {
        "uri": "/content/recipes/mvc",
        "title": "ASP.NET MVC",
        "content": "\nASP.NET MVC\n",
        "tags": [
            "replatforming",
            "mvc",
            "asp.net"
        ]
    },
    {
        "uri": "/content/recipes/sharepoint",
        "title": "Sharepoint",
        "content": "\nSharepoint\n",
        "tags": [
            "modernization"
        ]
    },
    {
        "uri": "/content/recipes/smb_file_share_access",
        "title": ".NET applications access to NFS (Prior to Volume Services)",
        "content": "\n\nNET application on PCF are running on Diego Windows Cell which is not joined any domain.\nAs a result connection to enterprise SMB shares requires authentication and is solved\nusing .NET wrapper around native Windows Networking API\n\nConfiguragtion\nInstall .NET Network library\n\ninstall-package Pivotal.Network.CloudFoundry\n\nUse Disposable NetworkConnection class to enable FileOperations on Shared Drives\n\nUsage:\n\n    using Pivotal.Network.CloudFoundry;\n\n    NetworkCredential credential = new NetworkCredential(user, password, domain);\n\n    using (NetworkConnection networkPath = new NetworkConnection(uncPath, credential))\n    {\n\t\t    //File Operations on the files/directories in uncPath here;\n        File ff = new File();\n\t  }\n \n [winapi]: https://msdn.microsoft.com/en-us/library/windows/desktop/aa385413(v=vs.85).aspx \"Windows Networking API\"\n [netlib]: https://github.com/pivotalservices/Manulife-App-Replatforming/tree/master/net-libraries/CloudFoundry-Network \"Network Library\"\n",
        "tags": [
            "replatforming",
            "mvc",
            "smb",
            "shares",
            "file",
            "windows"
        ]
    },
    {
        "uri": "/content/recipes/wcf_sso_clients",
        "title": "WCF Clients OAuth 2.0 and PCF SSO",
        "content": "\nIn the scenarios where APIs are consumed by other systems without involvement of user – batch processes, nightly schedules etc – Consumer application is authenticated using Oauth2.0 Client Credentials Flow – it provides the clientid, clientsecret to OIDC/Oauth2 provider and receives back signed Access token with the scopes this application is authorized for. This access token is passed in the HTTP header to the API for validation and authorization according to JWT Bearer Profile and Authorization profile\n\nSolution is based on WCF Client JWT Interceptor that will Connect PCF SSO and get Access Token and will embed it into HTTP Header for the service being invoked.\n\nHere is simplified diagram:\n\nWCF Services Clients\n \nJwt interceptor implements WCF  IClientMessageInspector  to get and inject the JWT token. To allow for configuration based setup behavior extension - Pivotal.Security.Jwt.JwtHeaderEndpointBehavior is provided and could be configured in web.config for the endpoints.  \n\n Configuration\n\nInstall the JWT Library that authenticates and gets Jwt Tokens  \ninstall-package Pivotal.Security.Jwt\nConfigure SSO – Configure SSO service on PCF for Service-to-Service scenario\n\nAdd scopes that need to be requested by the client\n\nappSettings\nadd key=\"RequiredScopes\" value=\"openid; yourapppermission_scope\" /\n/appSettings\n \nConfigure Jwt Behavior extension – Configure Jwt endpoint behavior which will get Access_Token and add it to the HTTP Headers before calling webservice\n\nsystem.serviceModel\nbehaviors\nendpointBehaviors\nbehavior name=\"jwtBehavior\"\n    jwtSSOBehavior /\n/behavior\n/endpointBehaviors\n/behaviors\nextensions\nbehaviorExtensions\nadd name=\"jwtSSOBehavior\" type=\"Pivotal.Security.Jwt.JwtHeaderEndpointBehavior, Pivotal.Security.Jwt\" /\n/behaviorExtensions\n/extensions\n\nclient\nendpoint address=\"http://<service.svc\" binding=\"basicHttpBinding\"\nbehaviorConfiguration=\"jwtBehavior\" /\n/client\n/system.serviceModel\n\n[pcfsso]: https://docs.pivotal.io/p-identity/1-3/configure-apps/web-app.html \"PCF SSO\"\n[jwtlib]: https://github.com/pivotalservices/Manulife-App-Replatforming/tree/master/net-libraries/CloudSecurity-JWT \"JWT Library\"\n",
        "tags": [
            "replatforming",
            "wcf",
            "SSO",
            "oauth"
        ]
    },
    {
        "uri": "/content/recipes/wcf_sso",
        "title": "WCF Services Security using OAuth 2.0 and PCF SSO",
        "content": "\nWCF WebServices (both SOAP and REST) are  protected by requiring all communication to present OAuth 2.0 Access Token.\nAccess Token obtained by the client from Pivotal SSO and is signed and encoded and is passed in  HTTP  Authorization Header according to JWT Bearer Profile and Authorization profile\nWCF JWT Interceptor will validate Access Token by retrieving the keys from PCF SSO and ensuring the JWT token is valid (signature, lifespan, scopes etc). If token is valid the Authenticated identity will be created and passed to the application context.\n\nHere is simplified diagram:\n\nType of the WCF application security checks\n\nAnalyze scenario which is used to invoke WCF Web Services - is there an end user involved or it's service to service (batch type) communication. In the later scope based Authorization is typically involved by validating scopes issued in the access token based on the client that is trying to connect.\nFor the former access token usually have some user specific information in the claims that helps to establish user role and perform Role based Authorization.\n\nTypically there are few types of Authorization checks:\n Principal Based Checks\nPerform entitlements checks based on the information available in the Principal for sceanrios with end user. For such scenarios code uses variations of the - IPrincial.IsInRole :\n\n      HttpContext.Current.User.IsInRole;\n      Thread.CurrentPrincipal.Identity;\n      ClaimsPrincipal.IsInRole;\n\nOr declaratively using Attributes\n\n      [PrincipalPermission(SecurityAction.Demand,Role=\"Admin\")]\n\nClaim Based checks -  claims/scope checks\n  For server to server (batch) scenarios there is no end User and communication is secured by Oauth2 token with scopes. Scopes represent claims about permissions that the consumer has based on it's authentication. There is no User Principal in context of the application in this case.\n \n  In code to obtain Identity usually done:\n\n   ClaimsIdentity identity = ClaimsPrincipal.Current.Identity as ClaimsIdentity;\n   ClaimsPrincipal principal = Thread.CurrentPrincipal as ClaimsPrincipal;\n   // check the scope\n   principal.HasClaim(\"scope\", specific scope);\n\n WCF Authentication and Authorization Solution\n  In WCF behaviors modify run-time behavior at the service level or at the endpoint level and will be utilized to create Custom Security behavior that would validate JWT token and necessary entitlements. Authorization decisions are made by the custom JwtServiceAuthorizationManager class; that validates the scopes provided in JWT.\n\nJWT based Authentication for Service-to-Service Claims based checks\n  Install the JWT Library that authenticates the Jwt Token and provides WCF extension points\n\n     install-package Pivotal.Security.Jwt\n     Setup resources and available scopes for the application in PCF SSO Tile\n    Obtain and configure authentication domain url - it will be used to retrieve UAA signing keys\n\n  Configure the WCF service Behavior -  to include the JWT Authentication  using Pivotal.Security.Jwt.JwtAuthorizationManager. It will act as interceptor by connecting to the configured PCF SSO authentication domain and retrieving JWT signing key and validating JWT provided in Authorization Header. If no header present of invalid JWT token - HTTP 401 error will be issued.\n\n  To be applied at All services that do not have explicit behavior configuration.\n    system.serviceModel\n  behaviors\n  serviceBehaviors\n   behavior\n\n     !--Claims with Roles--\n     <serviceAuthorization principalPermissionMode=\"Custom\"\n  serviceAuthorizationManagerType=\"Pivotal.Security.Jwt.JwtAuthorizationManager, Pivotal.Security.Jwt\" /\n    /behavior\n  /serviceBehaviors\n  /behaviors\n  /system.serviceModel\n  To apply it on per-Service basis you could create named behavior and apply it to service.\n\nserviceBehaviors\nbehavior name=\"JWTBehaviour\"\n!--Claims with Roles--\n<serviceAuthorization principalPermissionMode=\"Custom\"\nserviceAuthorizationManagerType=\"Pivotal.Security.Jwt.JwtAuthorizationManager, Pivotal.Security.Jwt\" /\n/behavior\n/serviceBehaviors\n...\nservice name=\"WcfTestService.TestRestService\" behaviorConfiguration=\"JWTBehaviour\" \nendpoint address=\"\" behaviorConfiguration=\"restfullBehavior\" binding=\"webHttpBinding\" contract=\"WcfTestService.ITestRestService\" /\n/service  \nConfigure Audience and scopes -  JWT obtained by clients will have audience elements listing its client_id  in it. Service should configure which clients(audiences it trusts) - they will be validated as part of Jwt validation process. For Authorization checks  Enable list of the Required Scopes to be used with ScopePermission Attribute or in code.\n\nappSettings\n!-- semicolon separated lists --\n   add key=\"AddAllowedAudiences\" value=\"<client id\" /\n   add key=\"RequiredScopes\" value=\"<scope name\"/\n/appSettings\n\nPerform AuthZ checks\n\n ClaimsPrincipal principal = Thread.CurrentPrincipal as ClaimsPrincipal;\n // check the scope\n principal.HasClaim(\"scope\", specific scope);\n\n [ScopePermission(SecurityAction.Demand, ConfigurationName = \"RequiredScopes\")]\n\n JWT based Authentication for End user scenario Role based checks\n\nInstall the package that authenticates the Jwt Token and retrieves AD groups  and sets them in the Principal\n\n   install-package Pivotal.Security.Jwt\n \n Setup resources and available scopes for the application in PCF SSO Tile\n   Obtain and configure authentication domain url - it will be used to retrieve UAA signing keys\n   And configure AD/LDAP information for getting AD Groups \n \n Configure the WCF service Behavior -  to include the JWT Authentication and AD group retrieval to the Principal using Pivotal.Security.Jwt.JwtADAuthorizationManager\n Globally for all services:\n\nsystem.serviceModel\nbehaviors\nserviceBehaviors\n behavior\n\n   !--Claims with Roles--\n   <serviceAuthorization principalPermissionMode=\"Custom\"\nserviceAuthorizationManagerType=\"Pivotal.Security.Jwt.JwtADAuthorizationManager, Pivotal.Security.Jwt\" /\n  /behavior\n/serviceBehaviors\n/behaviors\n/system.serviceModel\n\nTo apply it on per-Service basis you could create named behavior and apply it to service.\n\nserviceBehaviors\nbehavior name=\"JWTBehaviour\"\n!--Claims with Roles--\n<serviceAuthorization principalPermissionMode=\"Custom\"\nserviceAuthorizationManagerType=\"Pivotal.Security.Jwt.JwtADAuthorizationManager, Pivotal.Security.Jwt\" /\n/behavior\n/serviceBehaviors\n...\nservice name=\"WcfTestService.TestRestService\" behaviorConfiguration=\"JWTBehaviour\" \nendpoint address=\"\" behaviorConfiguration=\"restfullBehavior\" binding=\"webHttpBinding\" contract=\"WcfTestService.ITestRestService\" /\n/service\n\nExample of retrieving Identity and Claims, ACLs in WCF code\n\n// Get current identity the preferred way\nClaimsIdentity identity = ClaimsPrincipal.Current.Identity as ClaimsIdentity;\n// Get current identity the old apps way\nClaimsIdentity identity = Thread.CurrentPrincipal.Identity as ClaimsIdentity;\n// Get email from the identity\nstring emailClaim = identity.FindFirst(ClaimTypes.Email).Value;\n \n Declaratively test the permissions on  the service using Attribute on the service\nReplace the existing [PrincipalPermission] (or add new to the service you need to secure)\nwith the extended   PrincipalPermissionEnv attribute that will read  the role configuration from environment variable in PCF or web.config locally, instead of hardcoded roles\n\nappSettings\nadd key=\"AdminRole\" value=\"<ad group name\"/\n/appSettings\n\nusing Pivotal.Security.Jwt;\npublic class TestRestService : ITestRestService\n{\n[PrincipalPermissionEnv(SecurityAction.Demand, Role = \"AdminRole\")]\npublic string DoWork(string id)\n{\nClaimsIdentity identity = ClaimsPrincipal.Current.Identity as ClaimsIdentity;\n}\n}\n\nJWT Library Reference\n \nThe SSO integration is plugged to WCF extensibility point to apply Authentication/Authorization in the behavior for the services implementing custom ServiceAuthorizationManager:\n\nPivotal.Security.Jwt.JwtAuthorizationManager - implements the ServiceAuthorizationManager and validates the JWT token based on SSO jwt keys , validates  signature, lifetime, scopes, audience and issuer.\n \n \nPivotal.Security.Jwt.JwtADAuthorizationManager - extends JwtAuthorizationManager by querying AD and getting Roles into the Principal object for further evaluation\n\nPrincipalPermissionEnv – custom Attribute to validate Roles based on the configured in environment Roles. Works with JwtADAuthorizationManager configured as it checks the Principal roles.\n\n[PrincipalPermissionEnv(SecurityAction.Demand, Role = \"AdminRole\")]\n \nScopePermission - custom Attribute to validate Scopes based on the configured in environment settings.\n[ScopePermission(SecurityAction.Demand, ConfigurationName = \"RequiredScopes\")]\n \nError Handling\n \nJwt validation interceptor will return HTTP 401 code (with error code and error description in WWW-Authenticate Header) for authentication failures and authorization checks according to:\nhttps://tools.ietf.org/html/rfc6750\n \n\n[custbeh]: https://docs.microsoft.com/en-us/dotnet/framework/wcf/feature-details/security-behaviors-in-wcf \"Security Behaviors in WCF\"\n[pcfsso]: https://docs.pivotal.io/p-identity/1-3/configure-apps/web-app.html \"PCF SSO\"\n[servauthz]: https://docs.microsoft.com/en-us/dotnet/framework/configure-apps/file-schema/wcf/serviceauthorization-element \"Service Authorization\"\n[jwtlib]: https://github.com/pivotalservices/Manulife-App-Replatforming/tree/master/net-libraries/CloudSecurity-JWT \"JWT Library\"\n",
        "tags": [
            "replatforming",
            "wcf",
            "SSO",
            "oauth"
        ]
    },
    {
        "uri": "/content/recipes/wcf",
        "title": "WCF services recipes for PCF migration",
        "content": "\n\nWCF services as observed in the wild can typically run in Cloud Foundry with minimal changes. WCF is a large and complicated framework so the guidance here is for the most common type of configurations. The most common of these being SOAP or REST based services using an http(s) transport hosted either in IIS or self hosted.\n\nWCF services which are already hosted in IIS can usually be pushed to Cloud Foundry with very few changes, if any. The set of gotchas for a WCF service hosted in IIS are largely same as for any other ASP.NET application. Some common gotchas when hosting WCF services in Cloud Foundry are:\n\nWindows auth is not supported.\nX509 certificates cannot be stored in the Windows cert store.\nSSL offload/load balancing sometimes require custom WCF behaviors.\nNot all transports are supported.\nSelf hosted WCF services (e.g. hosted outside IIS).\n\nWindows Authentication\nSince Cloud Foundry cells and their containers are not domain joined, they cannot use Active Directory accounts for authenticating calls. An alternative option may be to use basic auth over https or certificate based authentication.\n\nTODO: Example with more specific guidance\n\n Certificate Authentication\nSince apps run under a non-priviledged container account they do not have access to install certs in the Windows cert store and have the WCF runtime automatically load the cert. To work around this, the app will need to dynamically load the cert from a location it has access too. One way is with a custom ServicePointManager integration that allow the app to load the cert from the local file system, env var, or secrets store and implement a validate method itself.\n\nTODO: Example with more specific guidance.\nImplementing a WCF Client with Certificate-Based Mutual Authentication without using Windows Certificate Store\n\nSSL Offload\nSSL offload will affect metadata generation for WCF SOAP services, so instead of getting the service's protocol, FQDN, and port, it'll use the internal IP and port of the container. To remedy this you'll need to create a custom endpoint behavior extension.\n\nTODO: Example with more specific guidance\nWCF: SSL offloading in load balancer – a simple approach as a starting point. TODO: specific guidance.\n\n Transports\nWCF supports a multiude of transport options, however Cloud Foundry does not support all available WCF transport options. The most common WCF transport is http(s) which fortunately is the bread and butter of Cloud Foundry. Other more esoteric transports like MSMQ and named pipes are not supported within Cloud Foundry. The TCP transport may work, however that needs to be verified.\n\nSelf Hosted WCF Services\nWCF has the ability to be self hosted or in other words hosted outside IIS. Self hosting is typically done programatically from a .NET Windows Service, although sometimes from console applications. Self hosting is not compatibile with Cloud Foundry when using an http(s) transport because the container doesn't have permissions to register a listener with Windows' HTTP.sys subsystem. This means self hosted WCF apps are not supported within Cloud Foundry, although it's possible that the TCP transport will work within Cloud Foundry - however that's not been verified.\n\nWhile migrating a self hosted WCF service to Cloud Foundry is more work than one that is already hosted in IIS, it's not a significant amount of work. Here's a sample WCF service with a separate project for hosting it in IIS or HWC in Cloud Foundry.\n",
        "tags": [
            "replatforming",
            "wcf"
        ]
    },
    {
        "uri": "/content/recipes/web_sso",
        "title": "Web Applications (Forms/MVC) security using OIDC/OAuth 2.0 and PCF SSO",
        "content": "\nTo secure web based application typically OpenID Connect (OIDC) implicit flow with authorization code grant is used. Web application is contacting OIDC Provider which directs to user to Authenticate against IdP and after successful user authentication receives Authorization code. Web Application then exchanges the authz code to Identity or/and Access token from OIDC provider.\n\nFor the applications running on PCF it is straightforward to use PCF SSO tile as it is the Standard OIDC SSO provider implementing required endpoints.  PCF SSO could be wired to different IdPs validating user credentials (in the example below it's ADFS, but could be LDAP or another OIDC ). All OIDC interactions are handled by OWIN middleware that is intercepting requests and acts on behalf of app to provide security.\n\nHere is simplified diagram:\n\nTo see more detailed SSO with ADFS flow refer to Detailed SSO flow\n\nOWIN Midddleware\n\nThe standard way to offload common code such as Authentication from the application functionality is creating\ninterceptor - OIDC/OAuth 2.0 OWIN Middleware - and wiring it the application.\n\nTo integrate PCF SSO OAuth 2.0 provider use Pivotal OWIN middleware as it takes care of PCF SSO specifics.  \nThis middleware implements the required interactions with OIDC provider – Pivotal SSO Tile – and handles callbacks/responses to abstract most of the security code from the application itself.\nAs it is OWIN  middleware the first step is to make  the application based on OWIN and running in integrated pipeline.\n \n OWIN-ize  application\nFollowing steps were tested with classic .NET Web Forms and MVC projects:\n\nAdjust Project properties\nMake sure VS project is set to use to use local IIS express server(not dev server)\nUpdate following properties:\n  Enable SSL (OIDC requires it)\n  Enable Anomymous Authentication\n  Disable Windows Authentication\n  Managed pipeline set to integrated\n  \n\nTurn off Windows in web.config\nsystem.web\n  authentication mode=\"None\" /\n/system.web\n\n Add Initialization Logic (Owin startup Class)\nImport OWIN Nuget Libraries into project\ninstall-package Owin\ninstall-package Microsoft.Owin.Host.SystemWeb\ninstall-package Microsoft.Owin.Security.OpenIdConnect\n-Add Owin Startup class using VS template: Add New Item - Search for OWIN Startup class\n It will generate template for the class Startup.cs, in MVC framework it could reside in App_Start folder\nMore details\nusing System.Security.Claims;\nusing Microsoft.Owin.Host;\nusing Pivotal.Owin.Security.OpenIDConnect;\nusing System.Net;\nusing System.Web;\nusing Owin;\nusing Microsoft.Owin;\n \n[assembly: OwinStartup(typeof(yourappnamespace.Startup))]\nnamespace yourappnamespace\n{\npublic class Startup\n{\npublic void Configuration(IAppBuilder app)\n{\n}\n}\n}\n\nYou should be able to run the app and verify that Startup is invoked.\n\nAdd OIDC/OAuth steps\nAnalyze the application and make sure to arrange unprotected area for Login, Access Denied and Home pages and Secured/protected area\n\nRegister web application in Pivotal SSO Tile by binding to the PCF SSO service instance\n\nInstall Pivotal OAuth/OIDC Owin Middleware\ninstall-package Pivotal.OIDC.Middleware\nAdd SSO configuration to Startup by adding information from bound SSO VCAPSSERVICES (clientid, client_secret and authentication domain)\n\nIn this example below - it will change the pipeline to use OpenId Connect and use for Login path  /Home/AuthorizeSSO which is unprotected and redirects the unauthenicated user to the IDP:\n\nusing System;\nusing Microsoft.Owin;\nusing Owin;\nusing Microsoft.Owin.Security;\nusing Microsoft.Owin.Security.Cookies;\n\nusing Microsoft.Owin.Host;\nusing Pivotal.Owin.Security.OpenIDConnect;\nusing System.Net;\nusing System.Web;\nusing System.Web.Security;\n\n[assembly: OwinStartup(typeof(your namespace.Startup))]\nnamespace your namespace {\n\n    public class Startup\n    {\n       public void Configuration(IAppBuilder app)\n        {\n            app.SetDefaultSignInAsAuthenticationType(\"ExternalCookie\");\n            app.UseCookieAuthentication(new CookieAuthenticationOptions\n            {\n                AuthenticationType = \"ExternalCookie\",\n                AuthenticationMode = AuthenticationMode.Active,\n                CookieName = \".AspNet.ExternalCookie\",\n                LoginPath = new PathString(\"/Home/AuthorizeSSO\"),\n                ExpireTimeSpan = TimeSpan.FromMinutes(5)\n             });\n\n \n            app.UseOpenIDConnect(new OpenIDConnectOptions()\n            {\n                ClientID = \"id....\",\n                ClientSecret = \"secret\",\n                AuthDomain = \"https://authdomain.com\",\n                AppHost = app url,\n                AppPort = 0\n\n            });\n        }\n      }\n    }\n\nFull Startup class example could be found at [ Example App] (https://github/pivotalservices/)  \n\nConfigure Login page(WebForms) or route (MVC) to challenge unauthenticated user and redirect to Pivotal SSP\nForms example \"PivotalSSO\" - is PCF Tile SSO Name configured in Middleware\n\npublic partial class Login : System.Web.UI.Page\n  {\n      protected void Page_Load(object sender, EventArgs e)\n      {\n          if (!Request.IsAuthenticated)\n          {\n              string returnurl = HttpContext.Current.Request.Params[\"ReturnUrl\"];\n              HttpContext.Current.GetOwinContext().Authentication.Challenge(\n                     new AuthenticationProperties { RedirectUri = returnurl },\n                     \"PivotalSSO\");\n          }\n      }\n  }\nMVC Route Example\n\nnamespace namespace\n{\n    public class HomeController : Controller\n    {\n\n        public async TaskActionResult Index()\n        {\n\n            return View();\n        }\n\n        public ActionResult About()\n        {\n            ViewBag.Message = \"Your application description page.\";\n\n            return View();\n        }\n\n        public ActionResult Contact()\n        {\n            ViewBag.Message = \"Your contact page.\";\n\n            return View();\n        }\n\n        private IAuthenticationManager AuthenticationManager\n        {\n            get\n            {\n                return HttpContext.GetOwinContext().Authentication;\n            }\n        }\n\n        [AllowAnonymous]\n        public ActionResult AuthorizeSSO()\n        {\n            return new ChallengeResult(\"PivotalSSO\", Url.Action(\"Index\",\"Secured\"));\n        }\n    }\n\n    internal class ChallengeResult : HttpUnauthorizedResult\n    {\n        public ChallengeResult(string provider, string redirectUri)\n        {\n            LoginProvider = provider;\n            RedirectUri = redirectUri;\n        }\n\n        public string LoginProvider { get; set; }\n        public string RedirectUri { get; set; }\n        public override void ExecuteResult(ControllerContext context)\n        {\n            var properties = new AuthenticationProperties { RedirectUri = RedirectUri };\n            context.HttpContext.GetOwinContext().Authentication.Challenge(properties, LoginProvider);\n        }\n    }\n  }\n\n Protect the web Application Areas\n\n In webforms Application Secure protected area in web.config by denying access to\n unauthenticated Users\n location path=\"<protected folder\"\n    system.web\n      authorization\n        deny users=\"?\" /\n      /authorization\n    /system.web\n  /location\n In MVC controllers add [Authorize] attribute to secure the route\n\nMake sure to disable FormsAuthentication\nsystem.webServer\n    modules runAllManagedModulesForAllRequests=\"true\"\n       remove name=\"FormsAuthentication\" /\n    /modules\n/system.webServer\n\nGet Token Claims\n\nTo get claims from the access/id token  get them from ClaimsIdentity\n\n   ClaimsIdentity claimsIdentity = (ClaimsIdentity)HttpContext.Current.User.Identity;\n   // Access claims\n   Response.Write(\"bUser Claims from JWT/b BR/\");\n   foreach (Claim claim in claimsIdentity.Claims)\n    {\n           Response.Write(claim.Type +\"BR/\");\n           Response.Write(claim.Value + \"BR/\");\n    }\n\n [Optional] WCF services over SSL\nIf the web forms application has WCF services in the same project you need to enable them to run over SSL.\nAdd following to web.config services configuration\nbindings\nwebHttpBinding\nbinding name=\"….\"\nsecurity mode=\"Transport\"\ntransport clientCredentialType =\"None\"/transport\n/security\n/binding\n/webHttpBinding\nbindings\n\n[Optional] ASP.NET application LDAP Groups authorization\nMany legacy .NET forms apps are using RoleManager ASP.NET role checks - Perform entitlements checks by using configured RoleManager\nRoles are not set on the Principal, but are verified by invoking configured RoleManager Provider which will verify the roles. In code it is usually done by invoking configured RoleManager:\n  Roles.IsUserInRole;\nTo perform Authorization based on ActiveDirectory Groups, we provided custom ActiveDirectoryRoleProvider that implements RoleProvider interface contract by overriding methods such as IsUserInRole,FindUsersInRole,GetAllRoles\n\nTo use this AD RoleManager Library:\nInstall\ninstall-package Pivotal.Security.Ldap\n\nConfigure web.config\nsystem.web\n  authentication mode=\"None\" /\n\n  roleManager defaultProvider=\"DirectoryServicesRoleProvider\" enabled=\"true\" cacheRolesInCookie=\"true\"\n    providers\n      add name=\"DirectoryServicesRoleProvider\" type=\"Pivotal.Security.Ldap.ActiveDirectoryRoleProvider, Pivotal.Security.Ldap\" LDAPService=\"...\" /\n    /providers\n  /roleManager\n/system.web\n\nCode and attribute usage\nbool isInRole = Roles.IsUserInRole(\"<role name\");\n// or\n[Authorize(Roles = \"role name\")]\n\nFor more details Custom Role Providers\n\n[oidc]: https://openid.net/specs/openid-connect-basic-1_0.html \"OIDC Implicit flow\"\n[pcfsso]: https://docs.pivotal.io/p-identity/1-3/configure-apps/web-app.html \"PCF SSO\"\n[websso]: https://docs.pivotal.io/p-identity/1-3/getting-started.htmlinstall \"Setup SSO\"\n[sso]:  /ssoflow/detailedsso_flow.png \"Detailed SSO Flow\"\n[owin]: http://www.c-sharpcorner.com/UploadFile/4b0136/introduction-of-owin-startup-class-in-visual-studio-2013-rc/ \"OWIN Startup class\"\n[oidcazure]: http://www.cloudidentity.com/blog/2014/07/24/protecting-an-asp-net-webforms-app-with-openid-connect-and-azure-ad/ \"OIDC for WebForms\"\n[roleprov]: https://www.codeproject.com/Articles/607392/Custom-Role-Providers \"Custom Role Providers\"\n[implroleprov]: https://msdn.microsoft.com/en-us/library/8fw7xh74.aspx \"Implementing Role Providers\"\n[ssolib]: https://github.com/pivotalservices/Manulife-App-Replatforming/tree/master/net-libraries/owin-oidc\n",
        "tags": [
            "replatforming",
            "mvc",
            "webforms",
            "SSO",
            "asp.net",
            "owin",
            "oidc"
        ]
    },
    {
        "uri": "/content/recipes/webapi",
        "title": "Web API",
        "content": "\nWeb API\n",
        "tags": [
            "replatforming",
            "api"
        ]
    },
    {
        "uri": "/content/recipes/webconfig",
        "title": "Web Config",
        "content": "\nWeb.config\n",
        "tags": [
            "replatforming",
            "modernization",
            "config",
            "asp.net"
        ]
    },
    {
        "uri": "/content/recipes/webforms",
        "title": "ASP.NET Webforms",
        "content": "\nASP.NET Webforms\n",
        "tags": [
            "replatforming",
            "webforms",
            "asp.net"
        ]
    },
    {
        "uri": "/content/recipes/windows_services",
        "title": "Windows NT Services",
        "content": "\nWindows Services written using .NET have become very common especially in conjunction with a front end ASP.NET application. Typically these types of applications poll a DB table for work or listen on an event queue to process work in an async fashion.\n\nWindows Services are not supported on PCF because they integate at the OS layer and duplicate and lot of the process life cycle management already built into PCF. However, the work load typically performed by Windows Services can usually be ported with minimal effort to PCF.\n\nReplatforming\n\nTaking an existing Windows Service and making it run on PCF usually involves a couple of steps, but the most important is changing how the .NET application is started.\n\nConvert the Windows Service to a Console Application.\nExternalize any configuration settings to environment variables or other externalized configuration strategies.\nSet the application health-check-type to process (available in PCF 1.10 and higher).\n\n Support Running as a Console App and a Windows Service\n\nSometimes you may need to support running the Windows Service the old way as a Windows Service while simultaneously supporting deployment to PCF. The best way to handle this is to use TopShelf to manage the Windows Service plumbing.\n\nTopShelf supports installing and running a console application as a Windows Service, or just running the app normally as a console application. This duality allows you to support both installation scenarios with minimal changes to the application code.\n\nEven if you don't plan on needing this long term, this technique can make for a reasonable method to iteratively deliver changes in the existing environment while supporting PCF replaforming.\n\nAdvanced Healthcheck\n\nWhile process level healthchecks are a good starting point and may be enough for some scenarios, it may make sense to implement a more in-depth custom health check in your worker service. This can be accomplished by using the port or http healthcheck.\n\nThe port healthcheck is pretty simple to setup, just listen on the $PORT specified by PCF if the application's self check tests pass. A further in depth test could be done by opening a TCP port and implementing a small listener the knows how to respond to HTTP requests. Here is a simple example.\n\nNOTE - You cannot directly use the .NET HTTP libraries because the process does not have CAS permissions to open a port. Opening a direct TCP port which doesn't use the HTTP.sys library works fine.\n",
        "tags": [
            "modernization",
            "services"
        ]
    }
]